{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19bda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta, bernoulli\n",
    "\n",
    "def beta_drop(inputs, a, b):\n",
    "    \n",
    "    x = tf.convert_to_tensor(inputs, name=\"x\")\n",
    "    x_dtype = x.dtype\n",
    "    \n",
    "    is_a_number = isinstance(a, numbers.Real)\n",
    "    is_b_number = isinstance(b, numbers.Real)\n",
    "    \n",
    "    if not tf.is_tensor(a) or not tf.is_tensor(b):\n",
    "        if is_a_number and is_b_number:\n",
    "            keep_prob = 1 - a/(a+b)\n",
    "            scale = 1 / keep_prob\n",
    "            scale = tf.convert_to_tensor(scale, dtype=x_dtype)\n",
    "            ret = tf.math.multiply(x, scale)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`a` and 'b' must be a scalar or scalar tensor. Received: a={a}, b={b}\")\n",
    "    else:\n",
    "        a.get_shape().assert_has_rank(0)\n",
    "        b.get_shape().assert_has_rank(0)\n",
    "        \n",
    "        a_dtype = a.dtype\n",
    "        b_dtype = b.dtype\n",
    "\n",
    "        if a_dtype != x_dtype or b_dtype != x_dtype:\n",
    "            if not a_dtype.is_compatible_with(x_dtype) or not b_dtype.is_compatible_with(x_dtype):\n",
    "                raise ValueError(\n",
    "                  \"`x.dtype` must be compatible with `a.dtype` and `b.dtype`. \"\n",
    "                  f\"Received: x.dtype={x_dtype} and a.dtype={a_dtype}, b.dtype={b_dtype}\")\n",
    "            a = tf.cast(a, x_dtype, name=\"a\")\n",
    "            b = tf.cast(b, x_dtype, name=\"b\")\n",
    "        one_tensor = tf.constant(1, dtype=x_dtype)\n",
    "        ret = tf.realdiv(x, tf.math.subtract(one_tensor, a/(a+b)))    \n",
    "    \n",
    "    size = x.shape[1:]    \n",
    "    \n",
    "    random_tensor = beta.rvs(a,b, size=size)\n",
    "    \n",
    "    random_tensor = tf.convert_to_tensor(random_tensor, dtype=x_dtype )\n",
    "    \n",
    "    ret = tf.math.multiply(ret, random_tensor)\n",
    "    \n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a030a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capa soft-dropout\n",
    "import numbers\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import torch\n",
    "\n",
    "from keras import backend\n",
    "from keras.engine import base_layer\n",
    "from keras.utils import control_flow_util\n",
    "\n",
    "class Soft_Dropout(base_layer.BaseRandomLayer):\n",
    "    \"\"\"Applies Dropout to the input.\n",
    "    Args:\n",
    "      a: Float higher than 0.\n",
    "      b: Float higher than 0.\n",
    "      noise_shape: 1D integer tensor representing the shape of the\n",
    "        binary dropout mask that will be multiplied with the input.\n",
    "        For instance, if your inputs have shape\n",
    "        `(batch_size, timesteps, features)` and\n",
    "        you want the dropout mask to be the same for all timesteps,\n",
    "        you can use `noise_shape=(batch_size, 1, features)`.\n",
    "      seed: A Python integer to use as random seed.\n",
    "    Call arguments:\n",
    "      inputs: Input tensor (of any rank).\n",
    "      training: Python boolean indicating whether the layer should behave in\n",
    "        training mode (adding dropout) or in inference mode (doing nothing).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, b, noise_shape=None, seed=None, **kwargs):\n",
    "        super().__init__(seed=seed, **kwargs)\n",
    "        if isinstance(a, (int, float)) and not 0 < a : #cambiar para condiciones de a y b > 0\n",
    "            raise ValueError(\n",
    "                f\"Invalid value {a} received for \"\n",
    "                \"`a`, expected a value higher than 0.\"\n",
    "            )\n",
    "        if isinstance(b, (int, float)) and not 0 < b : #cambiar para condiciones de a y b > 0\n",
    "            raise ValueError(\n",
    "                f\"Invalid value {b} received for \"\n",
    "                \"`b`, expected a value higher than 0.\"\n",
    "            )\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.scale = a/(a+b)\n",
    "        self.noise_shape = noise_shape\n",
    "        self.seed = seed\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def _get_noise_shape(self, inputs):\n",
    "        # Subclasses of `Dropout` may implement `_get_noise_shape(self,\n",
    "        # inputs)`, which will override `self.noise_shape`, and allows for\n",
    "        # custom noise shapes with dynamically sized inputs.\n",
    "        if self.noise_shape is None:\n",
    "            return None\n",
    "\n",
    "        concrete_inputs_shape = tf.shape(inputs)\n",
    "        noise_shape = []\n",
    "        for i, value in enumerate(self.noise_shape):\n",
    "            noise_shape.append(\n",
    "                concrete_inputs_shape[i] if value is None else value\n",
    "            )\n",
    "        return tf.convert_to_tensor(noise_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training is None:\n",
    "            training = backend.learning_phase()\n",
    "        \n",
    "        def dropped_inputs():\n",
    "            \n",
    "            return beta_drop(\n",
    "            inputs, self.a, self.b\n",
    "            )\n",
    "            \n",
    "        output = control_flow_util.smart_cond(\n",
    "            training, dropped_inputs,lambda: tf.identity(inputs)#lambda: tf.multiply(inputs, self.scale) # \n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"a\":self.a,\n",
    "            \"b\":self.b,\n",
    "            \"scale\":self.scale,\n",
    "            \"noise_shape\": self.noise_shape,\n",
    "            \"seed\": self.seed,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5571716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "class EarlyStoppingTresh(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    EarlyStoppingTresh Callback para detener la ejecucion cuando el valor monitor llegue al threshold\n",
    "    \n",
    "    :param monitor: metrica a monitorear\n",
    "    :param threshold: humbral, si se supera se detiene la ejecucion\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, monitor='loss',threshold=0.1):\n",
    "        super(EarlyStoppingTresh, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.monitor = monitor\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        metric = logs[self.monitor]\n",
    "        if metric <= self.threshold:\n",
    "            print('\\nEpoch %d: Reached threshold, terminating training' % (epoch))\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b272b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(directory, dist, sep=' ', multclass=False):\n",
    "    \"\"\"\n",
    "    load_data Carga toda la data de una distancia en el directorio especificado\n",
    "\n",
    "    :param directory: directorio donde se encuentran los archivos\n",
    "    :param dist: distancia de las simulaciones ['0.1', '3.23', '6.87', '10']\n",
    "    :param sep: separador que usan los archivos [' ', ',']\n",
    "    :param multclass: define si los labels son categoricos o binarios\n",
    "    :return: dos variables, x la data : y los labels de cada dato\n",
    "    \"\"\" \n",
    "    \n",
    "    signal = []\n",
    "    #files = glob.glob(directory+'/s11.2--LS220_'+dist+'kpc_sim*.txt')\n",
    "    \n",
    "    files = glob.glob(directory+'/s11.2--LS220_'+dist+'kpc_sim*.txt')\n",
    "    \n",
    "    for file in files:\n",
    "        # Use the pandas.read_csv() function to read the contents of the file\n",
    "        data = pd.read_csv(file, sep=sep, header=None)\n",
    "        data = data.values.T\n",
    "        #print(data.shape)\n",
    "        # Add the contents of the file to the numpy array\n",
    "        signal.append(data)\n",
    "    \n",
    "    \n",
    "    noise = []\n",
    "    files = glob.glob(directory+'/aLIGO_noise_2sec_sim*.txt')\n",
    "    for file in files:\n",
    "        # Use the pandas.read_csv() function to read the contents of the file\n",
    "        data = pd.read_csv(file, sep=sep, header=None)\n",
    "        #print(data.shape)\n",
    "        data = data.values.T\n",
    "        \n",
    "        # Add the contents of the file to the numpy array\n",
    "        noise.append(data)\n",
    "        \n",
    "    if len(noise) != len(signal):\n",
    "        if len(noise) > len(signal):\n",
    "            noise, n = train_test_split(noise,train_size=len(signal) )\n",
    "        else:\n",
    "            signal, n = train_test_split(signal,train_size=len(noise) )\n",
    "    \n",
    "    y1 = np.zeros(len(noise))\n",
    "    y2 = np.ones(len(signal))\n",
    "    \n",
    "    x = np.vstack((signal,noise))\n",
    "    \n",
    "    y = np.hstack((y1,y2))\n",
    "    \n",
    "    if multclass:\n",
    "        y = to_categorical(y, dtype =\"uint8\")\n",
    "    \n",
    "    if x.dtype == 'O':\n",
    "        x = x.astype(complex)\n",
    "        x = abs(x)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1b31f-6dc3-491b-8bdd-0fd5c79b4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizacion\n",
    "import numpy as np \n",
    "\n",
    "def norm_data(x):\n",
    "    \"\"\"\n",
    "    norm_data Normaliza la data de x\n",
    "    \n",
    "    :return: x normalizado\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0,len(x)):\n",
    "        dfmax, dfmin = np.max(x[i]), np.min(x[i])\n",
    "        x[i] = (x[i] - dfmin)/(dfmax - dfmin)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda1f85c-949e-4d8d-ab7f-03e0adfa6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficar las metricas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph_metrics(history, name, text = ''):\n",
    "    \"\"\"\n",
    "    graph_metrics Grafica las metricas: loss, val_loss, accuracy y val_accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    epochs = range(1,len(loss) + 1 )\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    \n",
    "    plt.plot(epochs, acc, 'b', label='Acc')\n",
    "    plt.plot(epochs, val_acc, 'purple', label='val_acc')\n",
    "    \n",
    "    plt.title(name)\n",
    "    plt.xlabel('Epochs \\n ' + text)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c40a7d-2084-4282-a4ac-c5968c1bc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from sklearn.utils import shuffle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def name_store(directory= 'data/timeFrecuency'):\n",
    "    \n",
    "    dist = ['0.1','5.05','10']\n",
    "    \n",
    "    for i in range( len(dist) ):\n",
    "        signal_names = glob.glob(directory+'/s11.2--LS220_'+dist[i]+'kpc_sim*.txt')\n",
    "        \n",
    "        noise_names = glob.glob(directory+'/aLIGO_noise_2sec_sim*.txt')\n",
    "\n",
    "        if len(noise_names) != len(signal_names):\n",
    "            if len(noise_names) > len(signal_names):\n",
    "                noise_names, n = train_test_split(noise_names,train_size=len(signal_names) )\n",
    "            else:\n",
    "                signal_names, n = train_test_split(signal_names,train_size=len(noise_names) )\n",
    "        \n",
    "        y1 = np.zeros(len(noise_names))\n",
    "        y2 = np.ones(len(signal_names))\n",
    "    \n",
    "        x_names = signal_names + noise_names\n",
    "    \n",
    "        y = np.hstack((y1,y2))\n",
    "    \n",
    "        y = to_categorical(y, dtype =\"uint8\")\n",
    "            \n",
    "        x_shuffled, y_shuffled = shuffle(x_names, y)\n",
    "            \n",
    "        outdir = directory + '_names_' + dist[i]\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        \n",
    "        X_train_filenames, X_val_filenames, y_train, y_val = train_test_split(\n",
    "            x_shuffled, y_shuffled, test_size=0.3, random_state=1)\n",
    "        \n",
    "        \n",
    "        name_x_t = outdir + '/train_list_dist_' + dist[i] + '.npy'\n",
    "        np.save(name_x_t, X_train_filenames)\n",
    "        \n",
    "        name_x_v = outdir + '/val_list_dist_' + dist[i] + '.npy'\n",
    "        np.save(name_x_v, X_val_filenames)\n",
    "        \n",
    "        name_y_t = outdir + '/train_labels_dist_' + dist[i] + '.npy'\n",
    "        np.save(name_y_t, y_train)\n",
    "        \n",
    "        name_y_v = outdir + '/val_labels_dist_' + dist[i] + '.npy'\n",
    "        np.save(name_y_v, y_val)\n",
    "        \n",
    "        #separar train y test\n",
    "        #https://medium.com/@mrgarg.rajat/training-on-large-datasets-that-dont-fit-in-memory-in-keras-60a974785d71\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c203422",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_store('D:/benja/Ondas/Code/data/timeFrecuency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a49950a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, image_filenames, labels, batch_size) :\n",
    "    self.image_filenames = image_filenames\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    \n",
    "    return np.array( [ np.loadtxt(str(file_name), delimiter=',' ).T for file_name in batch_x]), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2d930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
